---
title: 'Final Project: Food Inspection Failures in Chicago'
author: "Sabir Nazarov"
date: "12/1/2021"
output: 
  html_document: 
    self_contained: yes
    code_folding: hide
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---


# Introduction

stuff

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
	message = FALSE,
	warning = FALSE,
	include = TRUE,
	cache = FALSE,
	echo=TRUE)

library(tidyverse)
library(tidycensus)
library(dplyr)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

## Read in Data from Chicago

We .


```{r , results = FALSE}

# Create a map containing boundary of Chicago.
chicagoBoundary <- 
  st_read(file.path(root.dir,"/Chapter5/chicagoBoundary.geojson")) %>%
  st_transform('ESRI:102271') 

## Neighborhoods to use 
neighborhoods <- 
  st_read("https://raw.githubusercontent.com/blackmad/neighborhoods/master/chicago.geojson") %>%
  st_transform(st_crs(fishnet)) 

# Create a fishnet containing 500 by 500 foot squares over the boundary of Chicago.
fishnet <- 
  st_make_grid(chicagoBoundary,
               cellsize = 500, 
               square = TRUE) %>%
  .[chicagoBoundary] %>%            # <- MDH Added
  st_sf() %>%
  mutate(uniqueID = rownames(.))

FoodInspect <-
  read.socrata("https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5") %>%
    mutate(year = substr(inspection_date,1,4)) %>% filter(year == "2018") %>%
    # filter(results=="Fail") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Food_Inspect")


```


Census data follows

```{r ACS Data, results = FALSE, message=FALSE,warning=FALSE, cache = FALSE}
# Create a list of all 2019 acs 5-year variables
varlist_2018 <- load_variables(2019, "acs5", cache = TRUE)

# Census API key
census_api_key("94efffd19b56ad527e379faea1653ee74dc3de4a",overwrite = TRUE)


# Grab nine specific tract-level variables using the tidycensus "get_acs" command for each tract in Boulder County in 2019
tracts18 <- get_acs(geography = "tract",            
                    variables = c("B01001_001","B23025_004","B06011_001",
                                  "B06012_002","B02001_002","B25002_003",
                                  "B25013_006","B08013_001","B15012_009"), 
                    year=2018, 
                    state=17, 
                    county=031,
                    output = "wide",
                    geometry=TRUE) %>% 
            st_transform('ESRI:102271') %>%
            dplyr::select( c("GEOID","B01001_001E","B23025_004E","B06011_001E",
                      "B06012_002E","B02001_002E","B25002_003E",
                    "B25013_006E","B08013_001E","B15012_009E","geometry") ) %>%
            rename(tot_pop = "B01001_001E",
                   empl_pop = "B23025_004E",
                   med_inc = "B06011_001E",
                   pvty_pop = "B06012_002E",
                   white_pop = "B02001_002E",
                   vac_occ = "B25002_003E",
                   own_occ_bach = "B25013_006E",
                   tt_work = "B08013_001E",
                   sci_bach = "B15012_009E") %>%
            mutate(area = as.numeric(st_area(geometry)/1000000))%>%
            mutate(pop_den = tot_pop/area)

```


## Visualizing Theft Data

Stuff

```{r fig.width=6, fig.height=4}
# uses grid.arrange to organize indpendent plots
grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = chicagoBoundary) +
  geom_sf(data = FoodInspect, colour="red", size=0.1, show.legend = "point") +
  labs(title= "Food Inspections, Chicago - 2017") +
  mapTheme(title_size = 14),

ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "grey80") +
  stat_density2d(data = data.frame(st_coordinates(FoodInspect)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_viridis() +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Food Inspections") +
  mapTheme(title_size = 14) + theme(legend.position = "none"))


```



Inspections are then attributed to each fishnet cell.

```{r}
## add a value of 1 to each crime, sum them with aggregate
inspect_net <- 
  dplyr::select(FoodInspect) %>% 
  mutate(CountInspect = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countTheft = replace_na(CountInspect, 0),
         uniqueID = rownames(.),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = inspect_net, aes(fill = CountInspect), color = NA) +
  scale_fill_viridis() +
  labs(title = "Count of Inspections for the Fishnet") +
  mapTheme()

```

## Pulling Risk Factors

Stuff

```{r, results = FALSE}



## using Socrata again
RodentBait <- 
  read.socrata("https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Rodent-Baiting-Historic   al/97t6-zrhs") %>%
    mutate(year = substr(creation_date,1,4)) %>% filter(year == "2018") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Rodent_Bait")

Sanitation311 <-
  read.socrata(paste0("https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Sanitation-Code-Complaints-Hi/me59-5fac")) %>%
  mutate(year = substr(creation_date,1,4)) %>%
  filter(year == "2018") %>%
  filter(what_is_the_nature_of_this_code_violation_ %in%
  c("Garbage in Yard","Garbage in Alley","Dumpster not being emptied", "Overflowing carts", "Construction Site Cleanliness/Fence", "Standing water")) %>%
  dplyr::select(Y = latitude, X = longitude) %>%
  na.omit() %>%
  st_as_sf(coords = c("X","Y"), crs=4326, agr="constant") %>%
  st_transform(st_crs(fishnet)) %>%
  mutate(Legend = "Sanitation_311")


OrdinanceViolation <- 
  read.socrata("https://data.cityofchicago.org/Administration-Finance/Ordinance-Violations-Buildings-/awqx-tuwv") %>%
    mutate(year = substr(violation_date,1,4)) %>% filter(year == "2018") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Ordinance _Violations")


graffiti <-
  read.socrata(paste0("https://data.cityofchicago.org/Service",
  "-Requests/311-Service-Requests-Graffiti-Removal-Historical/",
  "hec5-y4x5")) %>%
  mutate(year = substr(creation_date,1,4)) %>%
  filter(year == "2017") %>%
  filter(where_is_the_graffiti_located_ %in%
  c("Front","Rear","Side")) %>%
  dplyr::select(Y = latitude, X = longitude) %>%
  na.omit() %>%
  st_as_sf(coords = c("X","Y"), crs=4326, agr="constant") %>%
  st_transform(st_crs(fishnet)) %>%
  mutate(Legend = "Graffiti")


# streetLightsOut <-
#   read.socrata(paste0("https://data.cityofchicago.org/Service",
#   "-Requests/311-Service-Requests-Street-Lights-All-Out/",
#   "zuxi-7xem")) %>%
#   mutate(year = substr(creation_date,1,4)) %>%
#   filter(year == "2017") %>%
#   dplyr::select(Y = latitude, X = longitude) %>%
#   na.omit() %>%
#   st_as_sf(coords = c("X","Y"), crs=4326, agr="constant") %>%
#   st_transform(st_crs(fishnet)) %>%
#   mutate(Legend = "Street_Lights_Out")


liquorRetail <-
  read.socrata(paste0("https://data.cityofchicago.org/resource/",
  "nrmj-3kcf.json")) %>%
  filter(business_activity ==
  "Retail Sales of Packaged Liquor") %>%
  dplyr::select(Y = latitude, X = longitude) %>%
  na.omit() %>%
  st_as_sf(coords = c("X","Y"), crs=4326, agr="constant") %>%
  st_transform(st_crs(fishnet)) %>%
  mutate(Legend = "Liquor_Retail")



    
    


```

The multi-map below shows risk factors on a map.

```{r}
vars_net <- rbind(abandonCars, graffiti, streetLightsOut, liquorRetail, rodentBait, foodInspectFail)   %>%
  st_join(., fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  full_join(fishnet, by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>%
  st_sf() %>%
  dplyr::select(-`<NA>`) %>%
  na.omit() %>%
  ungroup()


vars_net.long <-
  gather(vars_net, Variable, value, -geometry, -uniqueID)

vars <- unique(vars_net.long$Variable)
  mapList1 <- list()
  
for(i in vars){
  mapList1[[i]] <-
  ggplot() +
  geom_sf(data = filter(vars_net.long, Variable == i),
  aes(fill=value), colour=NA) +
  scale_fill_viridis(name="") +
  labs(title=i) +
  mapTheme()}
  
do.call(grid.arrange,c(mapList1, ncol=3,
top="Risk Factors by Fishnet"))


```


## Modeling Spatial Features

The next chunk of code converts risk factors to nearest neighbor distances.  This feature allows us to look at risk factors in a spatially "smoothed" manner.  For all risk factors we pick k=3, indicating that we want an average distance of nearest 3 occurrences of the event. The nearest neighbor plots follow:

```{r}
# convinience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid

## create NN from abandoned cars
vars_net <- vars_net %>%
    mutate(Abandoned_Cars.nn = nn_function(st_c(st_coid(vars_net)), 
                                           st_c(abandonCars),
                                           k = 3))

## create NN from graffiti
vars_net <- vars_net %>%
    mutate(graffiti.nn = nn_function(st_c(st_coid(vars_net)), 
                                           st_c(graffiti),
                                           k = 3))

## create NN from streetLights Out
vars_net <- vars_net %>%
    mutate(Street_Lights_Out.nn = nn_function(st_c(st_coid(vars_net)), 
                                           st_c(streetLightsOut),
                                           k = 3))

## create NN from liquorRetail
vars_net <- vars_net %>%
    mutate(Liquor_Retail.nn = nn_function(st_c(st_coid(vars_net)), 
                                           st_c(liquorRetail),
                                           k = 3))

## create NN from Rodent_Bait
vars_net <- vars_net %>%
    mutate(Rodent_Bait.nn = nn_function(st_c(st_coid(vars_net)), 
                                           st_c(rodentBait),
                                           k = 3))

## create NN from food_Inspect_Fail
vars_net <- vars_net %>%
    mutate(food_Inspect_Fail.nn = nn_function(st_c(st_coid(vars_net)),
                                           st_c(foodInspectFail),
                                           k = 3))


vars_net.long.nn <-
  dplyr::select(vars_net, ends_with(".nn")) %>%
  gather(Variable, value, -geometry)
  vars <- unique(vars_net.long.nn$Variable)

mapList2 <- list()

for(i in vars){
  mapList2[[i]] <-
  ggplot() +
  geom_sf(data = filter(vars_net.long.nn, Variable == i),
  aes(fill=value), colour=NA) +
  scale_fill_viridis(name="") +
  labs(title=i) +
mapTheme()}

do.call(grid.arrange,c(mapList2, ncol = 3,
top = "Nearest Neighbor risk Factors by Fishnet"))

#Distance to Loop to be used later
loopPoint <-
  filter(neighborhoods, name == "Loop") %>%
  st_centroid()
  vars_net$loopDistance =
  st_distance(st_centroid(vars_net), loopPoint) %>%
  as.numeric()

```

The plot below combines risk factors into a single map.

```{r}
## Visualize the NN feature
vars_net.long.nn <- 
  dplyr::select(vars_net, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)

ggplot() +
      geom_sf(data = vars_net.long.nn, aes(fill=value), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Risk Factors NN Distance") +
      mapTheme()
```

Since the counts were aggregated to each cell by `uniqueID` we can use that to join the counts to the fishnet.

```{r}
## important to drop the geometry from joining features
final_net <-
  left_join(crime_net, st_drop_geometry(vars_net), by="uniqueID") 

```

### Join in areal data

We use spatial joins to attach *centroids* of fishnets to polygon for neighborhoods and police districts. The map below shows the fishnet as police districts.

```{r}

final_net <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(neighborhoods, name), by = "uniqueID") %>%
    st_join(dplyr::select(policeDistricts, District), by = "uniqueID") %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

# for live demo
mapview::mapview(final_net, zcol = "District")
# mapview::mapview(final_net, zcol = "name.x")

```



```{r}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weigths
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)
```

### Plotting local Moran's I results

The grid map below shows the following four elements:

1) Count of thefts in Chicago
2) Local Morans I's: dispersal or clustering of theft relative to adjacent neighborhoods
3) P-value: statistical significance
4) Significant Hotspots: areas of significant theft activity in Chicago at 99% significance level

```{r}
## see ?localmoran
local_morans <- localmoran(final_net$countTheft, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net.localMorans <- 
  cbind(local_morans, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(countTheft = countTheft, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)


# varList1 <- list()
# 
# for(i in vars){
#   varList1[[i]] <-
#   ggplot() +
#     geom_sf(data = filter(final_net.localMorans, Variable==i),
#     aes(fill = Value), colour=NA) +
#     scale_fill_viridis(name="") +
#     labs(title=i) +
#     mapTheme() + theme(legend.position="bottom")}
# 
# do.call(grid.arrange,c(varList1, ncol = 2,
#   top = "Local Morans I statistics, Theft"))
  
```



```{r}
## This is just for plotting
vars <- unique(final_net.localMorans$Variable)
varList2 <- list()

for(i in vars){
  varList2[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList2, ncol = 4, top = "Local Morans I statistics, Theft"))
```

## Distance to Hot spot

Once again, we'll use the nearest neighbor function to find the distance to a hot spot location.

```{r}
# generates warning from NN
final_net <- final_net %>% 
  mutate(Theft.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>%
  mutate(Theft.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           Theft.isSig == 1))), 
                       k = 1))

```

### Plot NN distance to hot spot

```{r}
ggplot() +
      geom_sf(data = final_net, aes(fill=Theft.isSig.dist), colour=NA) +
      scale_fill_viridis(name="Theft.isSig.dist") +
      labs(title="Distance to Highly Significant Theft Hotspots") +
      mapTheme()
```
### Distance to the Loop

We'll likewise consider the distance to the main business area and the center of Chicago--the Loop. See map below.

```{r}
ggplot() +
    geom_sf(data = vars_net, aes(fill=loopDistance), colour=NA) +
    scale_fill_viridis(name="loopDistance") +
    labs(title="Distance to the Loop") +
    mapTheme()
```


### Correlations

The plots below shows correlations between various features of risk factors and theft in Chicago. The plot highlights that the presence of restaurants with inspection failures and retail liquor locations are potentially significant factors that contribute to theft.
```{r, fig.width=10, fig.height=20}
correlation.long <-
  st_drop_geometry(final_net) %>%
  dplyr::select(-uniqueID, -cvID, -name, -District, -loopDistance, -Theft.isSig, -Theft.isSig.dist) %>%
  gather(Variable, Value, -countTheft)


correlation.cor <-
  correlation.long %>%
  group_by(Variable) %>%
  summarize(correlation = cor(Value, countTheft, use = "complete.obs"))

ggplot(correlation.long, aes(Value, countTheft)) +
  geom_point(size = 0.1) +
  geom_text(data = correlation.cor,
  aes(label = paste("r =", round(correlation, 2))),
  x=-Inf, y=Inf, vjust = 1.5, hjust = -.1) +
geom_smooth(method = "lm", se = FALSE, colour = "black") +
facet_wrap(~Variable, ncol = 2, scales = "free") +
labs(title = "Theft count as a function of risk factors") +
plotTheme()


```

### Histogram of Dependent Variable
The histogram below provides a view of the frequency of the dependent variables.

```{r message=FALSE, warning=FALSE}
# calculate errors by NEIGHBORHOOD
HistogramTheft <- 
  final_net %>%
    group_by(cvID) %>% 
    summarize(countTheft_sum = sum(countTheft, na.rm = T),
              countTheft_mean = mean(countTheft, na.rm = T)) %>%
  ungroup()

# error_by_reg_and_fold %>% 
#   arrange(desc(MAE))
# error_by_reg_and_fold %>% 
#   arrange(MAE)

## plot histogram of theft
HistogramTheft %>%
  ggplot(aes(countTheft_sum)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
    scale_x_continuous(breaks = seq(30, 475, by = 25)) + 
    labs(title="Distribution of Theft Counts", subtitle = "Chicago, IL",
         x="Theft Count", y="Count") 


```

## Modeling

The model below uses all six risk factors as nearest neighbor functions, along with spatial features where theft is significant, as independent variables.  Count of theft is taken as the dependent variable. The model cycles through each neighborhood as a hold out, then trains on the remaining cells.  Cross-validation is then used to generate predictions for the hold out. 

We generate four variations of the model: risk factors with and without spatial features, k-fold and spatial cross-validation.  

```{r, results='hide'}

# View(crossValidate)

## define independent variables
reg.ss.vars <- c("Abandoned_Cars.nn", "graffiti.nn","food_Inspect_Fail.nn", "Liquor_Retail.nn", "Abandoned_Cars.nn", "Rodent_Bait.nn", "Street_Lights_Out.nn", "Theft.isSig", "Theft.isSig.dist", "loopDistance")


## define independent variables with spatial features
reg.vars <-
c("Abandoned_Cars.nn", "graffiti.nn","food_Inspect_Fail.nn", "Liquor_Retail.nn", "Abandoned_Cars.nn", "Rodent_Bait.nn", "Street_Lights_Out.nn")




## RUN REGRESSIONS
reg.ss.spatialCV <- crossValidate(
  dataset = dplyr::rename(final_net, countBurglaries = countTheft),
  id = "name",                           
  dependentVariable = "countBurglaries",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = name, countBurglaries, Prediction, geometry)


reg.cv <- crossValidate(
  dataset = dplyr::rename(final_net, countBurglaries = countTheft),
  id = "cvID",
  dependentVariable = "countBurglaries",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = cvID, countBurglaries, Prediction, geometry)


reg.ss.cv <- crossValidate(
  dataset = dplyr::rename(final_net, countBurglaries = countTheft),
  id = "cvID",
  dependentVariable = "countBurglaries",
  indVariables = reg.ss.vars) %>%
  dplyr::select(cvID = cvID, countBurglaries, Prediction, geometry)


reg.spatialCV <- crossValidate(
  dataset = dplyr::rename(final_net, countBurglaries = countTheft),
  id = "name",
  dependentVariable = "countBurglaries",
  indVariables = reg.vars) %>%
  dplyr::select(cvID = name, countBurglaries, Prediction, geometry)

```

### Cross-validation

Model errors are plotted in a histogram below.  The mode of mean absolute errors is 2 thefts.  The model can benefit from further refinement based on the presence of neighborhoods with significant errors.  Most grievous errors occur in neighborhoods near the Loop and the touristy Millennium Park, presumably areas with high prevelance of theft under $500.  

Comparing mean errors between models shows that spatial features improve our models significantly, compared to using just the risk factors.  The comparison map confirms this finding.

```{r}
reg.summary <-rbind(
  mutate(reg.cv, 
  Error = Prediction - countBurglaries,
  Regression = "Random k-fold CV: Just Risk Factors"),

  mutate(reg.ss.cv,
  Error = Prediction - countBurglaries,
  Regression = "Random k-fold CV: Spatial Process"),

  mutate(reg.spatialCV,
  Error = Prediction - countBurglaries,
  Regression = "Spatial LOGO-CV: Just Risk Factors"),

  mutate(reg.ss.spatialCV,
  Error = Prediction - countBurglaries,
  Regression = "Spatial LOGO-CV: Spatial Process")) %>%
st_sf()


# calculate errors by NEIGHBORHOOD
error_by_reg_and_fold <- 
  reg.summary %>%
  group_by(Regression, cvID) %>%
    summarize(Mean_Error = mean(Prediction - countBurglaries, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>% 
  arrange(desc(MAE))

# error_by_reg_and_fold %>% 
#   arrange(MAE)

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) +
  geom_histogram(bins = 30, colour="black", fill="#FDE725FF") +
  facet_wrap(~Regression) +
  geom_vline(xintercept = 0) + scale_x_continuous(
  breaks = seq(0, 8, by = 1)) +
  labs(title="Distribution of MAE",
  subtitle = "k-fold cross-validation vs. LOGO-CV",
  x="Mean Absolute Error", y="Count") +
plotTheme()


## Errors by Model
st_drop_geometry(error_by_reg_and_fold) %>%
  group_by(Regression) %>%
  summarize(Mean_MAE = round(mean(MAE), 2),
  SD_MAE = round(sd(MAE), 2)) %>%
  kable() %>%
  kable_styling("striped", full_width = F) %>%
  row_spec(2, color = "black", background = "#FDE725FF") %>%
  row_spec(4, color = "black", background = "#FDE725FF")


## map of errors by neighborhood
error_by_reg_and_fold %>%
  filter(str_detect(Regression, "LOGO")) %>%
  ggplot() +
  geom_sf(aes(fill = MAE)) +
  facet_wrap(~Regression) +
  scale_fill_viridis() +
  labs(title = "Theft errors by LOGO-CV Regression") +
mapTheme() + theme(legend.position="bottom")


## neighborhood weights

neighborhood.weights <-
  filter(error_by_reg_and_fold,
  Regression == "Spatial LOGO-CV: Spatial Process") %>%
  group_by(cvID) %>%
  poly2nb(as_Spatial(.), queen=TRUE) %>%
  nb2listw(., style="W", zero.policy=TRUE)
    filter(error_by_reg_and_fold, str_detect(Regression, "LOGO")) %>%
  st_drop_geometry() %>%
  group_by(Regression) %>%
  summarize(Morans_I =
  moran.mc(abs(Mean_Error), neighborhood.weights,
    nsim = 999, zero.policy = TRUE,
      na.action=na.omit)[[1]],
    p_value =
      moran.mc(abs(Mean_Error), neighborhood.weights,
    nsim = 999, zero.policy = TRUE,
    na.action=na.omit)[[3]])

```

## Density vs Predictions

Next we'll use kernel density with varying search radii: 1000, 1500 and 2000 feet.  Map below shows the visualization by 3 different search radii.


```{r}
# demo of kernel width
theft_ppp <- as.ppp(st_coordinates(theft), W = st_bbox(final_net))
theft_KD.1000 <- spatstat.core::density.ppp(theft_ppp, 1000)
theft_KD.1500 <- spatstat.core::density.ppp(theft_ppp, 1500)
theft_KD.2000 <- spatstat.core::density.ppp(theft_ppp, 2000)
theft_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(theft_KD.1000), as(neighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(theft_KD.1500), as(neighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(theft_KD.2000), as(neighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

theft_KD.df$Legend <- factor(theft_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=theft_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  mapTheme(title_size = 14)
```


Kernel density can be viewed on the fishnet with 1500 random theft observations.

```{r}

as.data.frame(theft_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value)) +
     geom_sf(data = sample_n(theft, 1500), size = .5) +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel density of 2017 thefts") +
     mapTheme(title_size = 14)
```

## Get 2018 crime data

To evaluate the generalizability of our model we'll apply our predictions and compare the results to 2018 theft statistics. 

```{r, results = FALSE}
theft18 <- 
  read.socrata("https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy") %>% 
  filter(Primary.Type == "THEFT" & Description == "$500 AND UNDER") %>%
  mutate(x = gsub("[()]", "", Location)) %>%
  separate(x,into= c("Y","X"), sep=",") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  na.omit %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271') %>% 
  distinct() %>%
  .[fishnet,]


```

### Generalizability across Race Contexts

ACS data allows us to evaluate generalizability by race contexts. The chunk below compares errors for majority white versus majority non-white neighborhoods.  Table below shows that we are generally under-predicting in non-white neighborhoods and over-predicting in majority white neighborhoods. Additionally, the model with spatial process generates smaller errors across neighborhood contexts.

```{r, include=FALSE}
tracts18 <-
  get_acs(geography = "tract",
  variables = c("B01001_001E","B01001A_001E"),
    year = 2018, state=17, county=031, geometry=T) %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(variable, estimate, GEOID) %>%
  spread(variable, estimate) %>%
  rename(TotalPop = B01001_001,
  NumberWhites = B01001A_001) %>%
    mutate(percentWhite = NumberWhites / TotalPop,
  raceContext = ifelse(percentWhite > .5,
    "Majority_White", "Majority_Non_White")) %>%
    .[neighborhoods,]


reg.summary %>%
  filter(str_detect(Regression, "LOGO")) %>%
  st_centroid() %>%
  st_join(tracts18) %>%
  na.omit() %>%
  st_drop_geometry() %>%
  group_by(Regression, raceContext) %>%
  summarize(mean.Error = mean(Error, na.rm = T)) %>%
  spread(raceContext, mean.Error) %>%
  kable(caption ="Mean Error by neighborhood racial context") %>%
kable_styling("striped", full_width = F)



```




```{r}
theft_KDE_sf <- as.data.frame(theft_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
  mutate(label = "Kernel Density",
         Risk_Category = ntile(value, 100),
         Risk_Category = case_when(
           Risk_Category >= 90 ~ "90% to 100%",
           Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
           Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
           Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
           Risk_Category >= 1 & Risk_Category  <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(theft18) %>% mutate(theftCount = 1), ., sum) %>%
    mutate(theftCount = replace_na(theftCount, 0))) %>%
  dplyr::select(label, Risk_Category, theftCount)
```



```{r}
theft_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category = ntile(Prediction, 100),
         Risk_Category = case_when(
         Risk_Category >= 90 ~ "90% to 100%",
         Risk_Category >= 70 & Risk_Category <= 89 ~ "70% to 89%",
         Risk_Category >= 50 & Risk_Category <= 69 ~ "50% to 69%",
         Risk_Category >= 30 & Risk_Category <= 49 ~ "30% to 49%",
         Risk_Category >= 1 & Risk_Category <= 29 ~ "1% to 29%")) %>%
  cbind(
    aggregate(
      dplyr::select(theft18) %>% mutate(theftCount = 1), ., sum) %>%
      mutate(theftCount = replace_na(theftCount, 0))) %>%
  dplyr::select(label,Risk_Category, theftCount)
```



```{r}
rbind(theft_KDE_sf, theft_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(theft18, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 theft risk predictions; 2018 theft") +
    mapTheme(title_size = 14)
```

Ultimately, we are able to compare our risk predictions versus kernel density of 2018 thefts.  We are especially interested in improvements in the high risk categories.  A well fit model will show the
highest risk categories capturing points of high density of theft.  In the table below we see that risk predictions are slightly higher than kernel density in the highest risk category of 90 to 100% (the rightmost bar).  This indicates that our model is able to capture the latent risk despite relatively few observed thefts.  This dynamic can also be observed in the medium risk category of 50 to 69%.  

With increased sophistication in engineered features we would expect our model to improve its ability to capture latent risk, thus increasing its predictive power.  An important consideration for a policymaker is the definite presence of racial bias in the risk factors.  It would not be prudent to "ignore" racial features in building the model with the hope of avoiding this bias, for the issue is baked into the data even if we exclude race variables.  This is due to a number of complicated social factors such as inconsistent reporting and enforcement across neighborhood characteristics.  Implicit bias will always be present and it is the job of the data scientist to discern and minimize its impact on the predictions.  One proposal made my data scientists at Yale is to include racial features in the training of the model but exclude it in predictions (Source:[Yale Insights](https://insights.som.yale.edu/insights/can-bias-be-eliminated-from-algorithms) .  



```{r}
rbind(theft_KDE_sf, theft_risk_sf) %>%
  st_set_geometry(NULL) %>% na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countTheft = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Rate_of_test_set_crimes = countTheft / sum(countTheft)) %>%
    ggplot(aes(Risk_Category,Rate_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE) +
      labs(title = "Risk prediction vs. Kernel density, 2018 thefts") +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```
